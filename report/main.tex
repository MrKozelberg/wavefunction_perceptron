\documentclass[11pt]{article}
\usepackage[total={170mm,230mm}]{geometry}

\usepackage{cmap}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}

\usepackage{breqn}

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{physics}
\usepackage{wrapfig}
\usepackage{cancel}
\usepackage{pdfpages}
\usepackage{hyperref}

\numberwithin{equation}{section}

\newtheorem{definition}{Опредление}[section]
\newtheorem{theorem}{Теорема}[section]
\newtheorem{axiom}{Аксиома}[section]

\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=newest}

\usepackage{amsmath}
\DeclareMathOperator\arctanh{arctanh}

\usepackage{amsmath}
\DeclareMathOperator\arccosh{arccosh}

\usepackage{amsmath}
\DeclareMathOperator\const{const}

\title{Применение многослойного перцептрона к решению многомерных задач квантовой механики}
\date{\today}
\begin{document}
\maketitle

\section{Вычисление оператора Лапласа для многослойного перцептрона}

\subsection{Математическое описание многослойного перцептрона}

Пускай на вход перцептрону подаётся вектор $\vb{x}\in\mathbb{R}^n$, он последовательно линейно преобразуется каждым слоем и в результате на выходе перцептрона имеется вектор $\vb{y}\in\mathbb{R}^m$. Пускай $l$~---~число внутренних слоёв нейронной сети, а на $k$-ом слое имеется $n_k$ нейронов ($n_0=n,\; n_{l+1}=m$). Через $\vb{h}^{(k)}\in\mathbb{R}^{n_k}$ будет обозначаться вектор на выходе с $k$-ого слоя. Используя введённые обозначения можно связать $\vb{x}$ и $\vb{y}$ следующим образом:
\begin{equation}
    \begin{split}
        h^{(0)}_i &= x_i,\quad i=\overline{1,n_0};\\
        h^{(1)}_i &= f^{(1)}\qty(\sum_{j=1}^{n_0} W^{(1)}_{i,j} h^{(0)}_j + b^{(1)}_i),\quad i=\overline{1,n_1};\\
        & \ldots\\
        h^{(k)}_i &= f^{(k)}\qty(\sum_{j=1}^{n_{k-1}} W^{(k)}_{i,j} h^{(k-1)}_j + b^{(k)}_i),\quad i=\overline{1,n_k};\\
        & \ldots\\
        h^{(l)}_i &= f^{(l)}\qty(\sum_{j=1}^{n_{l-1}} W^{(l)}_{i,j} h^{(l-1)}_j + b^{(l)}_i),\quad i=\overline{1,n_l};\\
        y_i = h^{(l+1)}_i &= f^{(l+1)}\qty(\sum_{j=1}^{n_{l}} W^{(l+1)}_{i,j} h^{(l)}_j + b^{(l+1)}_i),\quad i=\overline{1,n_{l+1}}.
    \end{split}
    \label{eq:perc}
\end{equation}
В предыдущей записи функция $f^{(k)}$ называются функцией активации $k$-го слоя, матрица $\hat W^{(k)}$~---~матрицей весов размера $n_k \times n_{k-1}$ (первый индекс~---~номер строки, второй~---~номер столбца), а вектор $b^{(k)}$~---~вектором сдвижки.

\subsection{Первая производная многослойного перцептрона}

Продифференцируем перцептрон по $x_t,\; t=\overline{1,n_0}$. Из \eqref{eq:perc} и правил взятия сложной производной следует такая схема расчёта
\begin{equation}
    \begin{split}
        &\pdv{y_i}{x_t} = \pdv{h^{(l+1)}_i}{x_t},\quad i=\overline{1,n_{l+1}};        
        \\
        & \pdv{h^{(k)}_i}{x_t} = f^{(k)\prime}\qty(\sum_{j=1}^{n_{k-1}} W^{(k)}_{i,j} h^{(k-1)}_j + b^{(k)}_i) \cdot \sum_{j=1}^{n_{k-1}} W^{(k)}_{i,j} \pdv{h^{(k-1)}_j}{x_t},\quad i=\overline{1,n_k},\; k = \overline{1, l+1};\\
        & \pdv{h^{(0)}_i}{x_t} = \delta_{i,t},\quad i=\overline{1,n_0}.
    \end{split}
\end{equation}
Можно заметить, что для вычисления производной разумно идти от нижних слоёв к верхним, так как вычисление каждой последующей $\pdv*{h^{(k)}_i}{x_t}$ зависит от значения таких производных на предыдущих слоях.

\subsection{Лапласиан многослойного перцептрона}

Основываясь на предыдущих результатах и полагая, что вектор $\vb{x}$ дан в $n$-мерной декартовой системе координат, нетрудно получить схему вычисления лапласиана перцептрона, которая имеет следующий вид:
\begin{equation}
    \laplacian y_i = \sum_{t=1}^{n_0} \pdv[2]{y_i}{x_t} = \sum_{t=1}^{n_0}\pdv[2]{h^{(l+1)}_i}{x_t},\quad i=\overline{1,n_{l+1}};
\end{equation}
\begin{equation}
    \begin{split}
        \pdv[2]{h^{(k)}_i}{x_t} = & f^{(k)\prime\prime}\qty(\sum_{j=1}^{n_{k-1}} W^{(k)}_{i,j} h^{(k-1)}_j + b^{(k)}_i) \cdot \qty{\sum_{j=1}^{n_{k-1}} W^{(k)}_{i,j} \pdv{h^{(k-1)}_j}{x_t}}^2\\
        &+ f^{(k)\prime}\qty(\sum_{j=1}^{n_{k-1}} W^{(k)}_{i,j} h^{(k-1)}_j + b^{(k)}_i) \cdot \sum_{j=1}^{n_{k-1}} W^{(k)}_{i,j} \pdv[2]{h^{(k-1)}_j}{x_t},\quad i=\overline{1,n_k},\; k = \overline{1, l+1};
    \end{split}
\end{equation}
\begin{equation}
    \pdv[2]{h^{(0)}_i}{x_t} = 0,\quad i=\overline{1,n_0}.
\end{equation}
При вычислении лапласиана, как и при вычислении градиента, следует начинать с нижних слоёв.


\subsection{Программная реализация и верификация}

Алгоритм вычисления градиента и лапласиана для многослойного перцептрона был реализован на языке Python с использованием библиотеки PyTorch. Вариант реализации и его верификация, описанная ниже, доступны по \href{https://github.com/MrKozelberg/laplacian_perceptron/blob/main/laplacian.ipynb}{ссылке}.

Для оценки верности работы алгоритмов был рассмотрен пример перцептрона с одним внутренним слоем, содержащим $n$ нейронов. На вход такой нейронной сети подавался вектор $\vb{x}\in\mathbb{R}^n$, что позволило выбрать веса следующими
\begin{equation}
 \hat W^{(1)} = \hat I,\quad \hat W^{(2)}_i = 1\; \forall i.
\end{equation}
Тогда нетрудно получить выражение результата работы перцептрона по входному вектору
\begin{equation}
 y(\vb{x}) = \tanh\qty(\sum_{k=1}^n\tanh\qty(x_k)).
\end{equation}
Отсюда можно получить выражение для градиента перцептрона
\begin{equation}
 \grad{y} (\vb{x})= \sum_{j=1}^n \cosh^{-2}\qty(\sum_{k=1}^n\tanh\qty(x_k)) \cosh^{-2}(x_j)\; \vb{e}_j,
\end{equation}
где вектора $\{ \vb{e}_j\}_{j=1}^n$ образуют стандартный базис в пространстве $\mathbb{R}^n$. Выражение для лапласиана функции $y(\vb{x})$ удаётся записать, используя предыдущие производные
\begin{equation}
\begin{split}
 \laplacian{y} (\vb{x}) = \sum_{i=1}^n \pdv{y}{x_i} \qty(\vb{x})\; \qty(\tanh\qty(x_i) + \dfrac{y(\vb{x})}{\cosh^2(x_i)}).
\end{split}
\end{equation}
Результаты тестирования доступны по \href{https://github.com/MrKozelberg/laplacian_perceptron/blob/main/laplacian.ipynb}{ссылке}. Из них можно заключить, что значение нейронной сети, её градиента и лапласиана вычисляются верно.

\section{Вычисление спектра N-мерного оператора Лапласа с помощью многослойного перцептрона}

\subsection{Квантово-механическая постановка задачи}

Рассмотрим стационарное уравнение Шрёдингера (УШ) для свободного электрона в N-мерной потенциальной яме с бесконечными стенками
\begin{equation}
 \hat H \, \psi(\vb{x}) = - \dfrac{\hbar^2}{2m}\laplacian \psi(\vb{x}) + V(\vb{x})\, \psi(\vb{x}) = E\, \psi(\vb{x}),
\end{equation}
где $\vb{x} \in \mathbb{R}^N$, а потенциал задаётся формулой
\begin{equation}
 V(\vb{x}) =
 \begin{cases}
  0, & x_i\in(0, L),\; i=\overline{1,N};\\
  +\infty, &\text{иначе}.
 \end{cases}
\end{equation}
Очевидно, что такая задача сводится к задаче отыскания спектра N-мерного оператора Лапласа с нулевыми граничными условиями на сторонах N-мерного куба
\begin{equation}
 -\laplacian \psi(\vb{x}) = \dfrac{2mE}{\hbar^2}\, \psi(\vb{x}),\quad \psi(\vb{x})\eval_{x_i=0,\, L} = 0,\; i=\overline{1,N}.
\end{equation}
Для краткости далее собственные значения оператора лапаласа будут обозначаться через $\lambda$, которые связаны с энергией состояния электрона соотношением
$$
\lambda = \dfrac{2mE}{\hbar^2}.
$$
В итоге получаем задачу на собственные значения и собственные функции
\begin{equation}
 \laplacian \psi(\vb{x}) = - \lambda \psi(\vb{x})
 \label{eq:laplacian}
\end{equation}
с граничными условиями
\begin{equation}
 \psi(\vb{x})\eval_{x_i=0,\, L} = 0,\; i=\overline{1,N}.
 \label{eq:boundary_condititon}
\end{equation}

\subsection{Неоднозначность подхода к решению задачи}

Предлагается найти состояние с наименьшим собственным значением уравнения \eqref{eq:laplacian} с граничными условиями \eqref{eq:boundary_condititon}, используя для приближения нужной собственной функции оператора Лаплaса перцептрон. Такая постановка задачи приводит к неоднозначному выбору функционала, минимум которого обеспечит наиболее подходящие параметры перцептрона, (то есть к неоднозначности выбора функции потерь) и к неоднозначности способа учёта граничных условий.

\subsubsection{Способы учёта граничных условий}

Учесть граничные условия \eqref{eq:boundary_condititon} можно несколькими способами. Рассмотрим некоторые из них. 
\begin{enumerate}
	\item Регуляризация волновой функции.
	
	Прежде всего можно регуляризовать волновую функцию $\psi$, рассмотрев её в виде произведения некоторой функции, удовлетворяющей граничным условиям, на перцептрон
	\begin{equation}
		\psi(\vb{x}) = A(\vb{x}) P(\vb{x}),\quad A(\vb{x})\eval_{x_i=0,\, L} = 0,\; i=\overline{1,N},
	\end{equation}
	где $P$~---~перцептрон.
	
	\item Регуляризация функции потерь.

	Можно также регуляризоваться и функцию потерь, добавив в неё член, минимальность которого обеспечивает минимальность значений волновой функции на границах рассматриваемой области. То есть предлагается перейти от функции потерь $I[\psi]$ к функции
	\begin{equation}
	 I[\psi] + \alpha \sum_{q=1}^{Q} \abs{\psi(\vb{x}^{(q)})},\quad \qty{\vb{x}^{(q)}}_{q=1}^Q \in \partial \Omega
	\end{equation}
	или к функции
	\begin{equation}
	 I[\psi] + \alpha \sum_{q=1}^{Q} \abs{\psi(\vb{x}^{(q)})}^2,\quad \qty{\vb{x}^{(q)}}_{q=1}^Q \in \partial \Omega,
	\end{equation}
	где $\Omega$~---~рассматриваемая область, $\partial \Omega$~---~её граница, а коэффициент $\alpha$ определяет важность учёта граничных условий по сравнению с тем, что учитывается в $I[\psi]$.
\end{enumerate}

\subsubsection{Варианты задания функции потерь}

Рассмотрим некоторые варианты задания функции потерь.

\begin{enumerate}
 \item Функционал энергии.

 В качестве функции потерь можно рассматривать функционал энергии $\mel{\psi}{\hat H}{\psi} / \ip{\psi}{\psi}$ (в нашем случае $\hat H = -\laplacian$). В случае многомерной задачи приближенно вычислять функционал энергии удобно с помощью метода Монте--Карло. Для этого поработаем с функцией потерь
 \begin{equation}
  \begin{split}
   I[\psi] &= \dfrac{\mel{\psi}{\hat H}{\psi}}{\ip{\psi}{\psi}} = \dfrac{\int \psi^* \hat H \psi \dd{\vb{x}}}{\int\abs{\psi}^2 \dd{\vb{x}}} = \int \dfrac{\abs{\psi}^2}{\int\abs{\psi}^2 \dd{\vb{x}}}\, \dfrac{\hat H \psi}{\psi} \dd{\vb{x}}\\
   &= \int P[\psi,\vb{x}]\, E_{\textrm{loc}}[\psi, \vb{x}] \dd{\vb{x}} \approx \dfrac{1}{M} \sum_{m=1}^M E_{\textrm{loc}}[\psi, \vb{x}^{(m)}],
  \end{split}
 \end{equation}
 где $P[\psi,\vb{x}] = {\abs{\psi}^2}/{\int\abs{\psi}^2 \dd{\vb{x}}}$~---~плотность вероятности, $E_{\textrm{loc}}[\psi, \vb{x}] = \hat H \psi / \psi$~---~локальная энергия, а приближенное равенство означает предположение о равномерности плотности вероятности $P[\psi,\vb{x}] = 1/{\int 1 \dd{\vb{x}}}$. Таким образом, в качестве функции потерь можно использовать среднее значение локальной энергии.

 \item Невязка.

 Можно ещё в качестве функции потерь использовать невязку, которая задаются как
 \begin{equation}
  I[\psi] = \sum_{m=1}^M \abs{(\hat H - E)\psi(\vb{x}^{(m)})}
  \label{eq:res_l1}
 \end{equation}
 или как
 \begin{equation}
  I[\psi] = \sum_{m=1}^M \abs{(\hat H - E)\psi(\vb{x}^{(m)})}^2,
 \end{equation}
 где в нашем случае $\hat H = -\laplacian$, $E$~---~\emph{заранее известный уровень энергии}, собственную функцию которого мы хотим найти. Такой подход подход подразумевает, что спектральная задача уже решена, и поэтому не может быть использован в решении задач со сложными гамильтонианами, спектр которых не известен.
\end{enumerate}

\subsection{Определение собственных волновых функций с априорным знанием спектра}
\label{subsec:test_laplac}

Попробуем найти собственные функции оператора Лапаласа по известным собственным значениям. Граничные условия учтём регуляризацией волновой функции, а в качестве регуляризующей фукнции возьмём функцию
\begin{equation}
 A(\vb{x}\in\mathbb{R}^N) = \prod_{i=1}^{N} \qty(\qty(x_i - \dfrac{1}{2})^2 - \dfrac{1}{4}).
\end{equation}
То есть волновая функция примет факторизованный вид:
\begin{equation}
 \psi(\vb{x}) = \mathcal{N}(\vb{x}) \, A(\vb{x}),
\end{equation}
где $\mathcal{N}(\vb{x})$~---~нейронная сеть. А в качестве функции ошибок возьмём невязку в виде \eqref{eq:res_l1}.

При задании некоторого перцептрона (выбор этих параметров пока плохо изучен и производился наобум) получилось вполне успешно решить такую тестовую задачу. Реализация и её результаты для размерностей $N=1,2,3$ могут быть найдены по \href{https://github.com/MrKozelberg/wavefunction_perceptron/blob/spectrum_laplace_operator/spectrum_laplace_operator_1.ipynb}{ссылке}.

\subsection{Отыскание спектра оператора Лапласа без его априорного знания}

Попробуем найти основное состояние многомерного оператора Лапласа в многомерном кубе с помощью многослойного перцептрона. Для этого снова будем учитывать граничные условия регуляризацией волнововой функции, как в разделе (\ref{subsec:test_laplac}). А в качестве функционала ошибок возьмём <<усовершенствованную>> невязку
\begin{equation}
	I_0[\psi] = \expval{ \qty(\laplacian \psi + \dfrac{\expval{-\psi\laplacian \psi}}{\expval{\abs{\psi}^2}} \psi)^2}.
\end{equation}
При этом энергия основного состояния определяется как
\begin{equation}
 E_0 [\psi] = \dfrac{\expval{-\psi\laplacian \psi}}{\expval{\abs{\psi}^2}}.
\end{equation}

Рассматриваемую задачу можно понимать как вариационную задачу на такую функцию $\psi_0$, которая бы обеспечивала минимум функционала $I_0[\psi]$, то есть
\begin{equation}
 \psi_0-?:\quad \min_{\psi} I_0[\psi] = I_0[\psi_0].
\end{equation}

Чтобы перейти от основного состояния к состоянию с индексом $1$ предлагается рассматривать задачу
\begin{equation}
 \psi_1-?:\quad \min_{\psi} I_1[\psi] = I_1[\psi_1],\quad I_1[\psi] = I_0[\psi-(\psi,\psi_0)\psi_0],
\end{equation}
при этом энергия состояния 1 определяется как
\begin{equation}
 E_1 [\psi] = \dfrac{\expval{-\psi\laplacian \psi}}{\expval{\abs{\psi}^2}}.
\end{equation}

\end{document}
